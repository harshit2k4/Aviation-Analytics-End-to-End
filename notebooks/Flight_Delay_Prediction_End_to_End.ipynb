{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epidYvkJbTH2"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path\n",
        "path = '/content/drive/MyDrive/dataset/flights.csv'\n",
        "\n",
        "# Load the dataset\n",
        "# Using low_memory=False because the dataset has mixed data types\n",
        "df = pd.read_csv(path, low_memory=False)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the binary target variable\n",
        "# 1 = Delayed (>15 mins), 0 = On Time (<=15 mins)\n",
        "df['Is_Delayed'] = (df['ARRIVAL_DELAY'] > 15).astype(int)\n",
        "\n",
        "# Check how many of each we have\n",
        "print(\"Target Variable Distribution:\")\n",
        "print(df['Is_Delayed'].value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "oFS-9DcDd85X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in each column\n",
        "missing_data = df.isnull().sum()\n",
        "print(\"Missing values per column:\\n\", missing_data[missing_data > 0])\n",
        "\n",
        "# Check for duplicate rows\n",
        "print(f\"\\nNumber of duplicate rows: {df.duplicated().sum()}\")"
      ],
      "metadata": {
        "id": "Gg7s3s-fewZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns that are mostly empty (over 90% missing values)\n",
        "limit = len(df) * 0.9\n",
        "df_cleaned = df.dropna(thresh=limit, axis=1)\n",
        "\n",
        "# Drop rows with missing values in critical columns\n",
        "critical_cols = ['DEPARTURE_TIME', 'DEPARTURE_DELAY', 'ARRIVAL_DELAY']\n",
        "df_cleaned = df_cleaned.dropna(subset=critical_cols)\n",
        "\n",
        "# Check how much data is left?\n",
        "print(f\"Original rows: {len(df)}\")\n",
        "print(f\"Cleaned rows: {len(df_cleaned)}\")\n",
        "print(\"\\nRemaining missing values:\")\n",
        "print(df_cleaned.isnull().sum().sum())"
      ],
      "metadata": {
        "id": "-9pXyNPYfQ_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Select only numerical columns for correlation\n",
        "numerical_df = df_cleaned.select_dtypes(include=[np.number])\n",
        "\n",
        "# Calculate absolute correlation with the target 'Is_Delayed'\n",
        "correlations = numerical_df.corr()['Is_Delayed'].abs().sort_values(ascending=False)\n",
        "\n",
        "# Get the top 10 features (excluding the target itself)\n",
        "top_10_features = correlations.iloc[1:11]\n",
        "\n",
        "print(\"Top 10 Features Selected by Correlation\")\n",
        "print(top_10_features)\n",
        "\n",
        "# Visualization for EDA Report\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=top_10_features.values, y=top_10_features.index, palette='viridis')\n",
        "plt.title('Top 10 Features Correlated with Flight Delays')\n",
        "plt.xlabel('Absolute Correlation Coefficient')\n",
        "plt.ylabel('Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6zIT2x3DguGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create a focused dataframe with only Top 10 features + the target\n",
        "selected_cols = list(top_10_features.index) + ['Is_Delayed']\n",
        "df_final = df_cleaned[selected_cols].copy()\n",
        "\n",
        "# Define Features (X) and Target (y)\n",
        "X = df_final.drop('Is_Delayed', axis=1)\n",
        "y = df_final['Is_Delayed']\n",
        "\n",
        "# Standardize the data (Mean=0, Variance=1)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"Data successfully transformed and scaled.\")\n",
        "print(f\"Final shape for modeling: {X_scaled.shape}\")"
      ],
      "metadata": {
        "id": "TdH7rNRehUzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "# To save time and memory, sample 100,000 rows from our 5.7M rows\n",
        "# This keeps the project manageable for SVM and KNN\n",
        "df_sample = df_final.sample(n=100000, random_state=42)\n",
        "\n",
        "X_sample = df_sample.drop('Is_Delayed', axis=1)\n",
        "y_sample = df_sample['Is_Delayed']\n",
        "\n",
        "# Balance Report (Before SMOTE)\n",
        "print(f\"Distribution before SMOTE: {Counter(y_sample)}\")\n",
        "\n",
        "# Apply SMOTE to balance the 17% delayed flights with the 83% on-time flights\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_sample, y_sample)\n",
        "\n",
        "# Balance Report (After SMOTE)\n",
        "print(f\"Distribution after SMOTE: {Counter(y_resampled)}\")"
      ],
      "metadata": {
        "id": "0NZ_FxvBhoFM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 2: Exploratory Data Analysis (EDA)"
      ],
      "metadata": {
        "id": "ZeM43mYAjLi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistical summary of the top 10 features\n",
        "print(\"Statistical Summary of Top 10 Features:\")\n",
        "display(df_cleaned[top_10_features.index].describe())"
      ],
      "metadata": {
        "id": "LXk1lwGmjTCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "# Limit to 200 minutes just to see the bulk of the data clearly\n",
        "sns.histplot(df_cleaned[df_cleaned['DEPARTURE_DELAY'] < 200]['DEPARTURE_DELAY'], bins=50, kde=True, color='royalblue')\n",
        "plt.title('Distribution of Departure Delays (Limited to < 200 mins)')\n",
        "plt.xlabel('Delay in Minutes')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ajCa0Y1yjlG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='DAY_OF_WEEK', y='Is_Delayed', data=df_cleaned, palette='magma')\n",
        "plt.title('Probability of Delay by Day of the Week')\n",
        "plt.ylabel('Proportion of Delayed Flights')\n",
        "plt.xlabel('Day of Week (1=Monday, 7=Sunday)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PdCjYM1tjybC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 10))\n",
        "# Calculate correlation for just the top 10 features\n",
        "top_corr = df_cleaned[top_10_features.index].corr()\n",
        "\n",
        "# Create a heatmap\n",
        "sns.heatmap(top_corr, annot=True, fmt=\".2f\", cmap='coolwarm', center=0)\n",
        "plt.title('Correlation Heatmap of Top 10 Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZBnwxZJrlnyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Phase 3: Model Building"
      ],
      "metadata": {
        "id": "ghodIYUanDPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the resampled data (from SMOTE step)\n",
        "# 80% for training, 20% for testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_resampled, y_resampled, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]} rows\")\n",
        "print(f\"Testing set size: {X_test.shape[0]} rows\")"
      ],
      "metadata": {
        "id": "TxnJA6QenFYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "# Initialize and Train\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Make Predictions\n",
        "y_pred_lr = lr_model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Logistic Regression Performance\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_lr))"
      ],
      "metadata": {
        "id": "HG6z7TuanRBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Initialize and Train\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make Predictions\n",
        "y_pred_nb = nb_model.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "print(\"Naive Bayes Performance\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_nb):.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_nb))"
      ],
      "metadata": {
        "id": "qGxTo2tWnX7J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}